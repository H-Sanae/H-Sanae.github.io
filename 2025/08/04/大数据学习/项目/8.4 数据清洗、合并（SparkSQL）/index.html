<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>医药项目：数据清洗、合并 | Tetuka的个人笔记</title><meta name="author" content="Tetuka"><meta name="copyright" content="Tetuka"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SparkSQLSpark SQL 是 Apache Spark 生态系统中的一个核心模块，专门用于处理结构化数据。它为用户提供了使用 SQL 语句或 DataFrame API 来查询和操作数据的能力，极大地简化了大数据分析任务的开发流程。 一、Spark SQL 的核心特点 统一的数据访问方式Spark SQL 支持从多种数据源读取数据，包括：  JSON、Parquet、ORC、CSV 等文">
<meta property="og:type" content="article">
<meta property="og:title" content="医药项目：数据清洗、合并">
<meta property="og:url" content="https://h-sanae.github.io/2025/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE/8.4%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E3%80%81%E5%90%88%E5%B9%B6%EF%BC%88SparkSQL%EF%BC%89/index.html">
<meta property="og:site_name" content="Tetuka的个人笔记">
<meta property="og:description" content="SparkSQLSpark SQL 是 Apache Spark 生态系统中的一个核心模块，专门用于处理结构化数据。它为用户提供了使用 SQL 语句或 DataFrame API 来查询和操作数据的能力，极大地简化了大数据分析任务的开发流程。 一、Spark SQL 的核心特点 统一的数据访问方式Spark SQL 支持从多种数据源读取数据，包括：  JSON、Parquet、ORC、CSV 等文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://h-sanae.github.io/img/peppapig7.png">
<meta property="article:published_time" content="2025-08-04T02:21:51.346Z">
<meta property="article:modified_time" content="2025-08-16T17:09:28.639Z">
<meta property="article:author" content="Tetuka">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://h-sanae.github.io/img/peppapig7.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "医药项目：数据清洗、合并",
  "url": "https://h-sanae.github.io/2025/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE/8.4%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E3%80%81%E5%90%88%E5%B9%B6%EF%BC%88SparkSQL%EF%BC%89/",
  "image": "https://h-sanae.github.io/img/peppapig7.png",
  "datePublished": "2025-08-04T02:21:51.346Z",
  "dateModified": "2025-08-16T17:09:28.639Z",
  "author": [
    {
      "@type": "Person",
      "name": "Tetuka",
      "url": "https://h-sanae.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/dbb.png"><link rel="canonical" href="https://h-sanae.github.io/2025/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE/8.4%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E3%80%81%E5%90%88%E5%B9%B6%EF%BC%88SparkSQL%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '医药项目：数据清洗、合并',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/dbb.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">69</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-flask"></i><span> 实验室</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="http://121.199.61.105/"><i class="fa-fw fas fa-q"></i><span> 豆瓣网开发</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fuckornot.on.websim.com/"><i class="fa-fw fa fa-trophy"></i><span> 上不上AI评分系统</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fontawesome.com/icons"><i class="fa-fw fa fa-check-circle"></i><span> font-awesome v6 图标</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/peppapig7.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/dbb.png" alt="Logo"><span class="site-name">Tetuka的个人笔记</span></a><a class="nav-page-title" href="/"><span class="site-name">医药项目：数据清洗、合并</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-flask"></i><span> 实验室</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="http://121.199.61.105/"><i class="fa-fw fas fa-q"></i><span> 豆瓣网开发</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fuckornot.on.websim.com/"><i class="fa-fw fa fa-trophy"></i><span> 上不上AI评分系统</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fontawesome.com/icons"><i class="fa-fw fa fa-check-circle"></i><span> font-awesome v6 图标</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">医药项目：数据清洗、合并</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-04T02:21:51.346Z" title="发表于 2025-08-04 10:21:51">2025-08-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-16T17:09:28.639Z" title="更新于 2025-08-17 01:09:28">2025-08-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><p>Spark SQL 是 Apache Spark 生态系统中的一个核心模块，专门用于处理结构化数据。它为用户提供了使用 SQL 语句或 DataFrame API 来查询和操作数据的能力，极大地简化了大数据分析任务的开发流程。</p>
<h3 id="一、Spark-SQL-的核心特点"><a href="#一、Spark-SQL-的核心特点" class="headerlink" title="一、Spark SQL 的核心特点"></a>一、Spark SQL 的核心特点</h3><ol>
<li><p><strong>统一的数据访问方式</strong><br>Spark SQL 支持从多种数据源读取数据，包括：</p>
<ul>
<li>JSON、Parquet、ORC、CSV 等文件格式</li>
<li>JDBC&#x2F;ODBC 接口连接传统数据库（如 MySQL、PostgreSQL）</li>
<li>Hive 表（通过 HiveContext）</li>
<li>Kafka 流数据（结合 Structured Streaming）</li>
</ul>
</li>
<li><p><strong>SQL 支持</strong><br>用户可以直接使用标准 SQL 查询数据，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, age <span class="keyword">FROM</span> people <span class="keyword">WHERE</span> age <span class="operator">&gt;</span> <span class="number">30</span></span><br></pre></td></tr></table></figure>
<p>这使得熟悉 SQL 的数据分析师可以轻松上手。</p>
</li>
<li><p><strong>DataFrame 和 Dataset API</strong><br>Spark SQL 提供了高层次的抽象：</p>
<ul>
<li><strong>DataFrame</strong>：以结构化方式组织的分布式数据集，类似于传统数据库中的表或 Pandas 中的 DataFrame。</li>
<li><strong>Dataset</strong>（仅在 Scala&#x2F;Java 中）：类型安全的 DataFrame，结合了对象模型和函数式编程的优势。</li>
</ul>
</li>
<li><p><strong>Catalyst 优化器</strong><br>Spark SQL 内置了基于规则和成本的查询优化器 Catalyst，能够自动优化 SQL 查询执行计划，例如谓词下推、列裁剪、常量折叠等，显著提升执行效率。</p>
</li>
<li><p><strong>Tungsten 引擎</strong><br>Tungsten 是 Spark 的高性能执行引擎，通过代码生成、内存管理和二进制格式优化，提升查询性能。</p>
</li>
<li><p><strong>与 Spark 生态无缝集成</strong><br>Spark SQL 可以与 Spark Streaming、MLlib（机器学习）、GraphX（图计算）等模块无缝协作，实现复杂的数据处理流水线。</p>
</li>
</ol>
<hr>
<h3 id="二、Spark-SQL-的架构"><a href="#二、Spark-SQL-的架构" class="headerlink" title="二、Spark SQL 的架构"></a>二、Spark SQL 的架构</h3><ol>
<li><strong>SQL Parser</strong>：解析 SQL 语句，生成抽象语法树（AST）。</li>
<li><strong>Analyzer</strong>：结合元数据（如 Hive Metastore 或内置 Catalog）解析未绑定的逻辑计划。</li>
<li><strong>Optimizer (Catalyst)</strong>：使用规则优化逻辑计划。</li>
<li><strong>Planner</strong>：将逻辑计划转换为多个物理执行计划。</li>
<li><strong>Cost-based Optimizer</strong>：选择最优的物理执行计划。</li>
<li><strong>Execution Engine</strong>：在集群上执行最终的物理计划。</li>
</ol>
<hr>
<h3 id="三、使用示例"><a href="#三、使用示例" class="headerlink" title="三、使用示例"></a>三、使用示例</h3><h4 id="1-使用-Spark-SQL-查询"><a href="#1-使用-Spark-SQL-查询" class="headerlink" title="1. 使用 Spark SQL 查询"></a>1. 使用 Spark SQL 查询</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&quot;SparkSQLExample&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 JSON 数据</span></span><br><span class="line">df = spark.read.json(<span class="string">&quot;people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册为临时视图</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 SQL 查询</span></span><br><span class="line">result = spark.sql(<span class="string">&quot;SELECT name, age FROM people WHERE age &gt; 30&quot;</span>)</span><br><span class="line">result.show()</span><br></pre></td></tr></table></figure>

<h4 id="2-使用-DataFrame-API"><a href="#2-使用-DataFrame-API" class="headerlink" title="2. 使用 DataFrame API"></a>2. 使用 DataFrame API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.<span class="built_in">filter</span>(df.age &gt; <span class="number">30</span>).select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="四、应用场景"><a href="#四、应用场景" class="headerlink" title="四、应用场景"></a>四、应用场景</h3><ul>
<li><strong>数据仓库查询</strong>：替代 Hive 做更高效的批处理查询。</li>
<li><strong>ETL 处理</strong>：清洗、转换、加载结构化数据。</li>
<li><strong>交互式分析</strong>：结合 BI 工具（如 Tableau、Superset）进行可视化分析。</li>
<li><strong>流批一体</strong>：通过 Structured Streaming 实现实时 SQL 查询。</li>
</ul>
<hr>
<h3 id="五、优势与局限"><a href="#五、优势与局限" class="headerlink" title="五、优势与局限"></a>五、优势与局限</h3><table>
<thead>
<tr>
<th>优势</th>
<th>局限</th>
</tr>
</thead>
<tbody><tr>
<td>高性能（得益于 Catalyst 和 Tungsten）</td>
<td>学习曲线较陡（尤其 Catalyst 内部机制）</td>
</tr>
<tr>
<td>支持标准 SQL 和多种数据源</td>
<td>相比专用数据库（如 ClickHouse），实时分析延迟仍较高</td>
</tr>
<tr>
<td>易于与 Spark 生态集成</td>
<td>资源消耗较大，适合中大规模数据处理</td>
</tr>
<tr>
<td>支持 Schema 演化和复杂数据类型（如 Array、Struct）</td>
<td>小文件处理效率较低</td>
</tr>
</tbody></table>
<hr>
<h3 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h3><p>Spark SQL 是大数据领域中最重要的 SQL 引擎之一，它将 SQL 的易用性与 Spark 的分布式计算能力相结合，广泛应用于数据湖分析、数据仓库、实时流处理等场景。对于需要处理 TB 到 PB 级结构化或半结构化数据的团队，Spark SQL 是一个强大而灵活的选择。在使用 Hadoop、Hive 或需要构建数据湖（Data Lake）这些场景下，Spark SQL 往往是首选的查询引擎之一。</p>
<hr>
<h1 id="项目实例"><a href="#项目实例" class="headerlink" title="项目实例"></a>项目实例</h1><h2 id="清洗数据（DWD层）"><a href="#清洗数据（DWD层）" class="headerlink" title="清洗数据（DWD层）"></a>清洗数据（DWD层）</h2><p>ODS &#x3D;&gt; DWD 数据清洗</p>
<p><strong>清洗流程图</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">   清洗规则--&gt;|可以修复|修复数据;</span><br><span class="line">   清洗规则--&gt;|无法修复|删除数据;</span><br><span class="line">   删除数据--&gt;非nmpa网站的数据;</span><br><span class="line">   修复数据--&gt;附件检测;</span><br><span class="line">   附件检测--&gt;下载成功;</span><br><span class="line">   附件检测--&gt;下载失败;</span><br><span class="line">   下载成功--&gt;存在附件文件;</span><br><span class="line">   存在附件文件--&gt;上传到HDFS-/zhiyun/huangwenzhe/attacments/nmpa/...;</span><br><span class="line">   存在附件文件--&gt;文本识别;</span><br><span class="line">   文本识别--&gt;word文档;</span><br><span class="line">   文本识别--&gt;pdf文档;</span><br><span class="line">   word文档--&gt;文本类型-现成的库处理;</span><br><span class="line">   pdf文档--&gt;文本类型-现成的库处理;</span><br><span class="line">   pdf文档--&gt;扫描类型-OCR识别;</span><br></pre></td></tr></table></figure>
<h3 id="识别附件内容追加到内容字段（content）"><a href="#识别附件内容追加到内容字段（content）" class="headerlink" title="识别附件内容追加到内容字段（content）"></a>识别附件内容追加到内容字段（content）</h3><p>提取word文字: <br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Yesi/p/17911301.html">https://www.cnblogs.com/Yesi/p/17911301.html</a><br><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_16175497/6731114">https://blog.51cto.com/u_16175497/6731114</a><br>Python 中可以读取 word 文件的库有 python-docx 和 pywin32。<br>python-docx跨平台只能处理 .docx 格式，不能处理.doc格式<br>pywin32仅限 windows 平台 .doc 和 .docx 都能处理<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/gs80140/article/details/145753312">python跨平台读取 .doc 格式文件靠谱的做法-CSDN博客</a></p>
<p>提取pdf文字: <br><a target="_blank" rel="noopener" href="https://blog.csdn.net/stephenzyx/article/details/129974065">https://blog.csdn.net/stephenzyx/article/details/129974065</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2323682">https://cloud.tencent.com/developer/article/2323682</a></p>
<h3 id="python清洗脚本"><a href="#python清洗脚本" class="headerlink" title="python清洗脚本"></a>python清洗脚本</h3><p><strong>yaozhi.py</strong> （仅处理yaozhi的数据，其他每个表各写一个py脚本，但总体结构差不多）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python3</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">from</span> PyPDF2 <span class="keyword">import</span> PdfReader  <span class="comment"># 处理PDF</span></span><br><span class="line"><span class="keyword">from</span> docx <span class="keyword">import</span> Document    <span class="comment"># 处理docx</span></span><br><span class="line"><span class="keyword">import</span> subprocess  <span class="comment">#处理doc</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 实例化 spark 会话</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .master(<span class="string">&quot;local[*]&quot;</span>) \</span><br><span class="line">    .appName(<span class="string">&quot;app&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&#x27;spark.driver.memory&#x27;</span>, <span class="string">&#x27;6g&#x27;</span>) \</span><br><span class="line">    .config(<span class="string">&#x27;spark.executor.memory&#x27;</span>, <span class="string">&#x27;6g&#x27;</span>) \</span><br><span class="line">    .config(<span class="string">&#x27;spark.local.dir&#x27;</span>, <span class="string">&#x27;/home/spark-tmp&#x27;</span>) \</span><br><span class="line">    .config(<span class="string">&#x27;spark.sql.parquet.writeLegacyFormat&#x27;</span>, <span class="string">&#x27;true&#x27;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://hdp:9083&quot;</span>) \</span><br><span class="line">    .enableHiveSupport() \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># UDF函数 用于提取每篇文章的平台ID</span></span><br><span class="line"><span class="meta">@F.udf(<span class="params">StringType(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_platform_id</span>(<span class="params">link</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> link:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分割路径获取最后一部分</span></span><br><span class="line">    parts = link.rstrip(<span class="string">&#x27;/&#x27;</span>).split(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> parts:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取文件名部分（如：3808585257548289.html）</span></span><br><span class="line">    filename = parts[-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 移除.html后缀并提取纯数字ID</span></span><br><span class="line">    <span class="keyword">if</span> filename.endswith(<span class="string">&#x27;.html&#x27;</span>):</span><br><span class="line">        id_part = filename[:-<span class="number">5</span>]  <span class="comment"># 移除5个字符（.html）</span></span><br><span class="line">        <span class="comment"># 验证剩余部分是否为纯数字</span></span><br><span class="line">        <span class="keyword">if</span> id_part.isdigit():</span><br><span class="line">            <span class="keyword">return</span> id_part</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>  <span class="comment"># 不符合预期格式时返回空字符串</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># UDF函数 用于提取附件文本内容</span></span><br><span class="line"><span class="meta">@F.udf(<span class="params">StringType(<span class="params"></span>)</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attachment_text</span>(<span class="params">attachment_path</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> attachment_path:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;未提供附件路径&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 1. 将Windows路径转为Linux路径</span></span><br><span class="line">        linux_path = re.sub(</span><br><span class="line">            <span class="string">r&#x27;^D:\\WorkSpace\\RPA_project\\attachments&#x27;</span>, </span><br><span class="line">            <span class="string">&#x27;/zhiyun/huangwenzhe/data/attachments&#x27;</span>, </span><br><span class="line">            attachment_path</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 将反斜杠替换为正斜杠</span></span><br><span class="line">        linux_path = linux_path.replace(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 检查Linux路径是否存在</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(linux_path):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;错误：路径不存在 - <span class="subst">&#123;linux_path&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 获取该路径下的所有附件 (.doc / .docx / .pdf)</span></span><br><span class="line">        attachments = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(linux_path):  <span class="comment"># 是目录</span></span><br><span class="line">            <span class="comment"># 查找目录下所有符合条件的文件</span></span><br><span class="line">            <span class="keyword">for</span> ext <span class="keyword">in</span> [<span class="string">&#x27;*.doc&#x27;</span>, <span class="string">&#x27;*.docx&#x27;</span>, <span class="string">&#x27;*.pdf&#x27;</span>]:</span><br><span class="line">                attachments.extend(glob.glob(os.path.join(linux_path, ext), recursive=<span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 是文件</span></span><br><span class="line">            <span class="keyword">if</span> linux_path.lower().endswith((<span class="string">&#x27;.doc&#x27;</span>, <span class="string">&#x27;.docx&#x27;</span>, <span class="string">&#x27;.pdf&#x27;</span>)):</span><br><span class="line">                attachments.append(linux_path)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> attachments:</span><br><span class="line">            <span class="comment"># print(f&quot;在路径 &#123;linux_path&#125; 下未找到任何附件文件&quot;)</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. 提取所有附件的文本内容</span></span><br><span class="line">        all_text = []</span><br><span class="line">        <span class="keyword">for</span> file_path <span class="keyword">in</span> attachments:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                file_ext = file_path.lower().split(<span class="string">&#x27;.&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">                filename = os.path.basename(file_path)</span><br><span class="line">                text = <span class="string">&quot;&quot;</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 处理Word文档 (.docx)</span></span><br><span class="line">                <span class="keyword">if</span> file_ext == <span class="string">&#x27;docx&#x27;</span>:</span><br><span class="line">                    doc = Document(file_path)</span><br><span class="line">                    full_text = []</span><br><span class="line">                    <span class="keyword">for</span> para <span class="keyword">in</span> doc.paragraphs:</span><br><span class="line">                        full_text.append(para.text)</span><br><span class="line">                    text = <span class="string">&#x27;\n&#x27;</span>.join(full_text)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 处理Word文档 (.doc) - 使用 catdoc</span></span><br><span class="line">                <span class="keyword">elif</span> file_ext == <span class="string">&#x27;doc&#x27;</span>:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        <span class="comment"># 打印调试信息</span></span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;尝试使用 catdoc 处理文件: <span class="subst">&#123;file_path&#125;</span>&quot;</span>)</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 使用 catdoc 提取文本</span></span><br><span class="line">                        result = subprocess.run(</span><br><span class="line">                            [<span class="string">&quot;catdoc&quot;</span>, <span class="string">&quot;-w&quot;</span>, <span class="string">&quot;0&quot;</span>, <span class="string">&quot;-d&quot;</span>, <span class="string">&quot;utf-8&quot;</span>, file_path],</span><br><span class="line">                            capture_output=<span class="literal">True</span>,</span><br><span class="line">                            text=<span class="literal">True</span>,</span><br><span class="line">                            encoding=<span class="string">&#x27;utf-8&#x27;</span>,</span><br><span class="line">                            errors=<span class="string">&#x27;ignore&#x27;</span></span><br><span class="line">                        )</span><br><span class="line">                        </span><br><span class="line">                        <span class="keyword">if</span> result.returncode == <span class="number">0</span>:</span><br><span class="line">                            text = result.stdout</span><br><span class="line">                            <span class="built_in">print</span>(<span class="string">f&quot;catdoc 处理完成: <span class="subst">&#123;result.stdout[:<span class="number">50</span>]&#125;</span>...&quot;</span>)</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            <span class="built_in">print</span>(<span class="string">f&quot;catdoc 处理失败 (<span class="subst">&#123;result.returncode&#125;</span>): <span class="subst">&#123;result.stderr&#125;</span>&quot;</span>)</span><br><span class="line">                            text = <span class="string">&quot;&quot;</span></span><br><span class="line">                    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;调用 catdoc 出错: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">                        text = <span class="string">&quot;&quot;</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 处理PDF文档</span></span><br><span class="line">                <span class="keyword">elif</span> file_ext == <span class="string">&#x27;pdf&#x27;</span>:</span><br><span class="line">                    reader = PdfReader(file_path)</span><br><span class="line">                    text = <span class="string">&quot;&quot;</span></span><br><span class="line">                    <span class="keyword">for</span> page <span class="keyword">in</span> reader.pages:</span><br><span class="line">                        page_text = page.extract_text()</span><br><span class="line">                        <span class="keyword">if</span> page_text:</span><br><span class="line">                            text += page_text + <span class="string">&quot;\n&quot;</span></span><br><span class="line">                </span><br><span class="line">                all_text.append(<span class="string">f&quot;[文件: <span class="subst">&#123;filename&#125;</span>]\n<span class="subst">&#123;text&#125;</span>\n&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[文件: <span class="subst">&#123;filename&#125;</span> 处理失败: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>]\n&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;\n&#x27;</span>.join(all_text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;处理附件时出错: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取Hive表数据</span></span><br><span class="line">sql = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    id,</span></span><br><span class="line"><span class="string">    &quot;&quot; as platform_id,</span></span><br><span class="line"><span class="string">    title,</span></span><br><span class="line"><span class="string">    dept,</span></span><br><span class="line"><span class="string">    post_date,</span></span><br><span class="line"><span class="string">    zihao,</span></span><br><span class="line"><span class="string">    level,</span></span><br><span class="line"><span class="string">    timeliness,</span></span><br><span class="line"><span class="string">    content,</span></span><br><span class="line"><span class="string">    link,</span></span><br><span class="line"><span class="string">    attachment_path,</span></span><br><span class="line"><span class="string">    create_time,</span></span><br><span class="line"><span class="string">    update_time</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">from ods_huangwenzhe.yaozhi</span></span><br><span class="line"><span class="string">;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df = spark.sql(sql)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 更新platform_id列</span></span><br><span class="line">df = df.withColumn(<span class="string">&quot;platform_id&quot;</span>, get_platform_id(<span class="string">&quot;link&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新content列，追加附件内容</span></span><br><span class="line">df = df.withColumn(</span><br><span class="line">    <span class="string">&quot;content&quot;</span>, </span><br><span class="line">    F.concat(</span><br><span class="line">        F.col(<span class="string">&quot;content&quot;</span>), </span><br><span class="line">        F.lit(<span class="string">&quot;\n\n[附件内容开始]\n\n&quot;</span>), </span><br><span class="line">        get_attachment_text(<span class="string">&quot;attachment_path&quot;</span>),</span><br><span class="line">        F.lit(<span class="string">&quot;\n\n[附件内容结束]&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line">df.show()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 写入到Hive表</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建路径</span></span><br><span class="line">os.system(<span class="string">&quot;hadoop fs -mkdir -p /zhiyun/huangwenzhe/dwd/yaozhi&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建数据库</span></span><br><span class="line">os.system(<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hive -e &quot;create database if not exists dwd_huangwenzhe location &#x27;/zhiyun/huangwenzhe/dwd&#x27;&quot;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 保存到hive表里</span></span><br><span class="line">df.write.<span class="built_in">format</span>(<span class="string">&quot;hive&quot;</span>) \</span><br><span class="line">    .mode(<span class="string">&quot;overwrite&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;fileFormat&quot;</span>,<span class="string">&quot;parquet&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;location&quot;</span>,<span class="string">&quot;/zhiyun/huangwenzhe/dwd/yaozhi&quot;</span>). \</span><br><span class="line">    saveAsTable(<span class="string">&quot;dwd_huangwenzhe.yaozhi&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n清洗完成&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="合并数据（DWS层）"><a href="#合并数据（DWS层）" class="headerlink" title="合并数据（DWS层）"></a>合并数据（DWS层）</h2><p>DWD &#x3D;&gt; DWS 宽表加工</p>
<p>要求: 把清洗好的各平台数据表合并成一张大表, 该表应该包含数据表的所有字段</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 无需执行，供参考</span></span><br><span class="line"><span class="keyword">create table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dws_huangwenzhe.wide_raw_table(</span><br><span class="line">id <span class="type">int</span>;</span><br><span class="line">platform_id string, <span class="comment">-- 平台的ID, 如果没有则需要从URL提取ID</span></span><br><span class="line">title string, <span class="comment">-- 标题 不能为空</span></span><br><span class="line">source string, <span class="comment">-- 来源（dept、source）</span></span><br><span class="line">post_date string,  <span class="comment">--发布日期</span></span><br><span class="line">category string, <span class="comment">-- 分类（level、category）</span></span><br><span class="line">timeliness string, <span class="comment">-- 状态  合并状态字段 timeliness</span></span><br><span class="line">content string, <span class="comment">-- 文本内容  不能为空</span></span><br><span class="line">link string, <span class="comment">-- 文章地址</span></span><br><span class="line">create_time string <span class="comment">--创建时间</span></span><br><span class="line">update_time string <span class="comment">--更新时间</span></span><br><span class="line">) partitioned <span class="keyword">by</span> (platform string);</span><br></pre></td></tr></table></figure>

<p><strong>bash脚本</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">set</span> -e  <span class="comment"># 任何命令执行失败立即退出脚本，避免错误累积</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志输出函数，方便追踪执行过程</span></span><br><span class="line"><span class="function"><span class="title">log</span></span>() &#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;[<span class="subst">$(date +&#x27;%Y-%m-%d %H:%M:%S&#x27;)</span>] <span class="variable">$1</span>&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 确保HDFS存储路径存在（避免表创建时路径权限问题）</span></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;创建宽表HDFS存储路径...&quot;</span></span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> -p /zhiyun/huangwenzhe/dws/wide_raw_table</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建数据库（确保数据库存在）</span></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;创建dws层数据库...&quot;</span></span><br><span class="line">hive -e <span class="string">&quot;</span></span><br><span class="line"><span class="string">create database if not exists dws_huangwenzhe </span></span><br><span class="line"><span class="string">location &#x27;/zhiyun/huangwenzhe/dws&#x27;;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建宽表（若不存在）</span></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;创建宽表结构...&quot;</span></span><br><span class="line">hive -e <span class="string">&quot;</span></span><br><span class="line"><span class="string">create table if not exists dws_huangwenzhe.wide_raw_table(</span></span><br><span class="line"><span class="string">    id int,</span></span><br><span class="line"><span class="string">    platform_id string,</span></span><br><span class="line"><span class="string">    title string,</span></span><br><span class="line"><span class="string">    source string,</span></span><br><span class="line"><span class="string">    post_date string,</span></span><br><span class="line"><span class="string">    category string,</span></span><br><span class="line"><span class="string">    timeliness string,</span></span><br><span class="line"><span class="string">    content string,</span></span><br><span class="line"><span class="string">    link string,</span></span><br><span class="line"><span class="string">    create_time string,</span></span><br><span class="line"><span class="string">    update_time string</span></span><br><span class="line"><span class="string">) partitioned by (platform string)</span></span><br><span class="line"><span class="string">stored as parquet</span></span><br><span class="line"><span class="string">location &#x27;/zhiyun/huangwenzhe/dws/wide_raw_table&#x27;;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理yaozhi平台数据</span></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;插入yaozhi数据到宽表...&quot;</span></span><br><span class="line">hive -e <span class="string">&quot;</span></span><br><span class="line"><span class="string">insert overwrite table dws_huangwenzhe.wide_raw_table partition(platform=&#x27;yaozhi&#x27;)</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    id,</span></span><br><span class="line"><span class="string">    platform_id,</span></span><br><span class="line"><span class="string">    title,</span></span><br><span class="line"><span class="string">    dept as source,</span></span><br><span class="line"><span class="string">    post_date,</span></span><br><span class="line"><span class="string">    level as category,</span></span><br><span class="line"><span class="string">    timeliness,</span></span><br><span class="line"><span class="string">    content,</span></span><br><span class="line"><span class="string">    link,</span></span><br><span class="line"><span class="string">    create_time,</span></span><br><span class="line"><span class="string">    update_time</span></span><br><span class="line"><span class="string">from dwd_huangwenzhe.yaozhi</span></span><br><span class="line"><span class="string">where title is not null and content is not null;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理nmpa平台数据</span></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;插入nmpa数据到宽表...&quot;</span></span><br><span class="line">hive -e <span class="string">&quot;</span></span><br><span class="line"><span class="string">insert overwrite table dws_huangwenzhe.wide_raw_table partition(platform=&#x27;nmpa&#x27;)</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    id,</span></span><br><span class="line"><span class="string">    index_id as platform_id,</span></span><br><span class="line"><span class="string">    title,</span></span><br><span class="line"><span class="string">    null as source,</span></span><br><span class="line"><span class="string">    post_date,</span></span><br><span class="line"><span class="string">    category,</span></span><br><span class="line"><span class="string">    null as timeliness,</span></span><br><span class="line"><span class="string">    content,</span></span><br><span class="line"><span class="string">    link,</span></span><br><span class="line"><span class="string">    create_time,</span></span><br><span class="line"><span class="string">    update_time</span></span><br><span class="line"><span class="string">from dwd_huangwenzhe.nmpa</span></span><br><span class="line"><span class="string">where title is not null and content is not null;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理cmde平台数据</span></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;插入cmde数据到宽表...&quot;</span></span><br><span class="line">hive -e <span class="string">&quot;</span></span><br><span class="line"><span class="string">insert overwrite table dws_huangwenzhe.wide_raw_table partition(platform=&#x27;cmde&#x27;)</span></span><br><span class="line"><span class="string">select</span></span><br><span class="line"><span class="string">    id,</span></span><br><span class="line"><span class="string">    platform_id,</span></span><br><span class="line"><span class="string">    title,</span></span><br><span class="line"><span class="string">    source,</span></span><br><span class="line"><span class="string">    post_date,</span></span><br><span class="line"><span class="string">    null as category,</span></span><br><span class="line"><span class="string">    null as timeliness,</span></span><br><span class="line"><span class="string">    content,</span></span><br><span class="line"><span class="string">    link,</span></span><br><span class="line"><span class="string">    create_time,</span></span><br><span class="line"><span class="string">    update_time</span></span><br><span class="line"><span class="string">from dwd_huangwenzhe.cmde</span></span><br><span class="line"><span class="string">where title is not null and content is not null;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">log</span> <span class="string">&quot;所有平台数据合并到宽表成功！&quot;</span></span><br></pre></td></tr></table></figure>
<p>若想提高插入效率可将三个查询语句用union all拼接，此处为了方便排错调试故拆开</p>
<h2 id="自动化调度"><a href="#自动化调度" class="headerlink" title="自动化调度"></a>自动化调度</h2><p>使用海豚调度平台将以上内容整合成一个工作流定时执行<br>调度平台：<a target="_blank" rel="noopener" href="http://192.168.8.67:12345/dolphinscheduler/ui/login">海豚调度平台</a></p>
<img src="/2025/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE/8.4%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E3%80%81%E5%90%88%E5%B9%B6%EF%BC%88SparkSQL%EF%BC%89/file-20250805155610394.png" class="">

<h3 id="结果验证"><a href="#结果验证" class="headerlink" title="结果验证"></a>结果验证</h3><img src="/2025/08/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE/8.4%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E3%80%81%E5%90%88%E5%B9%B6%EF%BC%88SparkSQL%EF%BC%89/file-20250805154732104.png" class=""></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post-share"><div class="social-share" data-image="/img/peppapig7.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Hadoop/6.24%20Hadoop%E9%83%A8%E7%BD%B2/" title="6.24 Hadoop部署"><img class="cover" src="/img/peppapig8.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">6.24 Hadoop部署</div></div><div class="info-2"><div class="info-item-1">Hadoop简介 Hadoop是一个由Apache基金会所开发的分布式系统基础架构 主要解决，海量数据的存储和海量数据的分析计算问题 广义上来说，Hadoop通常是指一个更广泛的概念-Hadoop生态圈  分布式存储Hadoop 的分布式存储主要基于 HDFS（分布式文件系统）:HDFS将数据分割成多个数据块（block），这些数据块分散存储在集群中的不同节点上。每个数据块会有多个副本，通常默认是 3 个副本.采用分布式存储在不同的节点上，提高了数据的可靠性和容错性。 Hadoop的分布式核心组件是MapReduce编程模型:在MapReduce任务中，数据被切分为多个任务，每个任务由或多个节点并行。每个节点负责将输入数据映射为键-值对生成中间结果。最后，中间结果按照键的排序进行合并和归并。 Hadoop组件HDFSHDFS组件用于存储数据,主要由NameNode,DataNode,SecondaryNameNode 组成  NameNode (nn): 存储文件的元数据，如文件名，文件目录结构，文件属性 (生成时间、副本数、文件权限)，以及每个文件的块列表和块所在的DataNo...</div></div></div></a><a class="pagination-related" href="/2025/06/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Hadoop/6.27%20Hive%E5%A4%8D%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E3%80%81%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0/" title="Hive复合数据类型、相关函数"><img class="cover" src="/img/peppapig5.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-27</div><div class="info-item-2">Hive复合数据类型、相关函数</div></div><div class="info-2"><div class="info-item-1">Hive复合数据类型通过以下三种复合类型，Hive 能够高效处理半结构化数据（如日志、JSON），避免频繁进行表连接操作 array 数组array&lt;value数据类型&gt;存储同类型元素的有序集合，元素通过索引访问（从 0 开始） 12345--查询复合数据select  a_score[0] from student2--构造复合数据-arrayselect array(值,值) from student struct 集合struct&lt;key值:value数据类型,key值:value数据类型&gt;存储不同类型字段的集合，每个字段有名称和类型，通过点号（.）访问 12345--查询复合数据select  s_score.chinese from student2--构造复合数据select named_struct(key,value,key,value) from student map 字典map&lt;key数据类型,value数据类型&gt;存储键值对（Key-Value）集合，键必须唯一，通过键访问值 12345--查询复合数据select  m_...</div></div></div></a><a class="pagination-related" href="/2025/06/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Hadoop/6.30%20Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E3%80%81%E5%88%86%E6%A1%B6%E8%A1%A8/" title="Hive分区表、分桶表"><img class="cover" src="/img/peppapig6.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-30</div><div class="info-item-2">Hive分区表、分桶表</div></div><div class="info-2"><div class="info-item-1">分区表分区是将一个表或索引物理地分解为多个更小、更可管理的部分。分区对应用透明，即对访问数据库的应用而言，逻辑上讲只有一个表或一个索引（相当于应用“看到”的只是一个表或索引），但在物理上这个表或索引可能由数十个物理分区组成。 在 Hadoop 中，Hive 分区表通常以特定的目录结构来存储。每个分区对应一个独立的目录，目录名通常包含分区列的值。数据文件会存储在相应的分区目录下。 分桶表•分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。 •分桶是将数据集分解成更容易管理的若干部分的另一个技术。 •分区针对的是数据的存储路径；分桶针对的是数据文件。 分桶策略：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方 式决定该条记录存放在哪个桶当中。     分区表 分桶表    划分依据 分区列的离散值（目录分割） 分桶列的哈希值（文件分割）   数量灵活性 分区数量可动态新增 桶数量固定（创建时指定）   优化目标 减少扫描范围（过滤查询） 优化 Join...</div></div></div></a><a class="pagination-related" href="/2025/07/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Hadoop/7.7%20hadoop%E9%97%AE%E7%AD%94/" title="hadoop问答"><img class="cover" src="/img/peppapig3.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-07</div><div class="info-item-2">hadoop问答</div></div><div class="info-2"><div class="info-item-1">1. hadoop组件有哪些?分别有什么功能   组件 功能    HDFS 分布式文件存储系统，提供高容错性海量存储   MapReduce 分布式计算框架，并行处理大数据集   YARN 资源调度系统，管理集群资源并分配任务   Hive 主要用于处理结构化和半结构化数据   Common 通用工具库，支持其他模块   2. 分布式存储的组件是什么 ?有哪些进程? 进程的作用是什么?分布式存储组件是 HDFS，其包含的进程及作用如下：    进程 作用    NameNode 管理元数据（文件名、块位置、权限），响应客户端请求   DataNode 存储实际数据块，定期向NN发送心跳和块报告   SecondaryNameNode 定期合并FsImage和Edits日志（非热备，缓解NN压力）   3. 资源调度使用什么组件? 该组件有哪些进程?每个进程的作用是什么?资源调度使用的组件是 YARN，相关进程及作用如下：    进程 作用    ResourceManager 负责全局资源的调度和分配   NodeManager 管理单个节点上的资源和容器   Applicat...</div></div></div></a><a class="pagination-related" href="/2025/06/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Hadoop/6.25%20Hive%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AEbeeline/" title="6.25 Hive安装、配置beeline"><img class="cover" src="/img/peppapig2.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-25</div><div class="info-item-2">6.25 Hive安装、配置beeline</div></div><div class="info-2"><div class="info-item-1">安装mysql12345678910111213141516#下载安装源wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm# 安装 mysql 源yum localinstall mysql57-community-release-el7-11.noarch.rpm# 导入keyrpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022# 修改国内源vim /etc/yum.repos.d/mysql-community.repo修改 baseurl 为 https://mirrors.cloud.tencent.com/mysql/yum/mysql-5.7-community-el7-x86_64/#安装mysqlyum install -y mysql-community-server  安装Hive12345678910111213#解压apache-hive-3.1.2-bin.tar.gz到/opt/module/...</div></div></div></a><a class="pagination-related" href="/2025/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Hadoop/6.26%20Python%E8%BF%9E%E6%8E%A5Hive/" title="Python连接Hive"><img class="cover" src="/img/peppapig3.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-26</div><div class="info-item-2">Python连接Hive</div></div><div class="info-2"><div class="info-item-1">hadoop上传数据12345hadoop dfs -mkdir /emphadoop dfs -put emp0901.txt /emp/hadoop dfs -mkdir /studenthadoop dfs -put student2.csv /student/  处理student2.txt表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import jsondef transform_student_data(input_file, output_file):    &quot;&quot;&quot;    将JSON格式的学生数据转换为CSV格式    参数:    input_file: 输入JSON数据文件路径    output_file: 输出CSV文件路径    &quot;&quot;&quot;    try:        with open(input_file, &#x27;r&#x27;, encoding=&#x...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/dbb.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Tetuka</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">69</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/H-Sanae/H-Sanae.github.io" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1466753498@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL"><span class="toc-text">SparkSQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81Spark-SQL-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9"><span class="toc-text">一、Spark SQL 的核心特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Spark-SQL-%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-text">二、Spark SQL 的架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-text">三、使用示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BD%BF%E7%94%A8-Spark-SQL-%E6%9F%A5%E8%AF%A2"><span class="toc-text">1. 使用 Spark SQL 查询</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BD%BF%E7%94%A8-DataFrame-API"><span class="toc-text">2. 使用 DataFrame API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">四、应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E4%BC%98%E5%8A%BF%E4%B8%8E%E5%B1%80%E9%99%90"><span class="toc-text">五、优势与局限</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">六、总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E5%AE%9E%E4%BE%8B"><span class="toc-text">项目实例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B8%85%E6%B4%97%E6%95%B0%E6%8D%AE%EF%BC%88DWD%E5%B1%82%EF%BC%89"><span class="toc-text">清洗数据（DWD层）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%86%E5%88%AB%E9%99%84%E4%BB%B6%E5%86%85%E5%AE%B9%E8%BF%BD%E5%8A%A0%E5%88%B0%E5%86%85%E5%AE%B9%E5%AD%97%E6%AE%B5%EF%BC%88content%EF%BC%89"><span class="toc-text">识别附件内容追加到内容字段（content）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python%E6%B8%85%E6%B4%97%E8%84%9A%E6%9C%AC"><span class="toc-text">python清洗脚本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E6%95%B0%E6%8D%AE%EF%BC%88DWS%E5%B1%82%EF%BC%89"><span class="toc-text">合并数据（DWS层）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%8C%96%E8%B0%83%E5%BA%A6"><span class="toc-text">自动化调度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E9%AA%8C%E8%AF%81"><span class="toc-text">结果验证</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/SQL/8.20%20sql%E9%A2%98%E5%A4%8D%E7%9B%98/" title="8.20 sql题复盘"><img src="/img/peppapig5.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="8.20 sql题复盘"/></a><div class="content"><a class="title" href="/2025/08/20/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/SQL/8.20%20sql%E9%A2%98%E5%A4%8D%E7%9B%98/" title="8.20 sql题复盘">8.20 sql题复盘</a><time datetime="2025-08-21T01:06:06.917Z" title="更新于 2025-08-21 09:06:06">2025-08-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Python/request%E5%92%8Cselenium%EF%BC%88Python%E7%88%AC%E8%99%AB%EF%BC%89/" title="request和selenium（Python爬虫）"><img src="/img/peppapig4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="request和selenium（Python爬虫）"/></a><div class="content"><a class="title" href="/2025/08/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/Python/request%E5%92%8Cselenium%EF%BC%88Python%E7%88%AC%E8%99%AB%EF%BC%89/" title="request和selenium（Python爬虫）">request和selenium（Python爬虫）</a><time datetime="2025-08-20T06:38:16.201Z" title="更新于 2025-08-20 14:38:16">2025-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/SQL/7.9%20SQL%E4%BC%98%E5%8C%96%E3%80%81Oracle%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E7%BB%83%E4%B9%A0/" title="SQL优化、Oracle复杂查询练习"><img src="/img/peppapig8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SQL优化、Oracle复杂查询练习"/></a><div class="content"><a class="title" href="/2025/07/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/SQL/7.9%20SQL%E4%BC%98%E5%8C%96%E3%80%81Oracle%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E7%BB%83%E4%B9%A0/" title="SQL优化、Oracle复杂查询练习">SQL优化、Oracle复杂查询练习</a><time datetime="2025-08-17T09:00:28.524Z" title="更新于 2025-08-17 17:00:28">2025-08-17</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/peppapig7.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By Tetuka</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.0-b2</a></span></div><div class="footer_custom_text">感谢阅读</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="输入以搜索内容..." type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>